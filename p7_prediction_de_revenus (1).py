# -*- coding: utf-8 -*-
"""P7_Prediction de revenus

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GezSaeuO7OvhiHXeyHZn5d_fvkBHKCsr

# Menu
"""

## definition du dossier drive
from google.colab import drive
drive.mount('/content/drive')

"""# Imports des librairies"""

!pip install pandas-profiling==2.7.1

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
from pandas_profiling import ProfileReport
import scipy.stats as st
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
plt.close('all')
from collections import Counter
from math import *
import time
import statsmodels.formula.api 
import statsmodels.api as sm
import seaborn as sns
import scipy
import scipy.stats as st
from scipy.stats import t, shapiro, ks_2samp
from statsmodels.stats.outliers_influence import variance_inflation_factor

data_raw = pd.read_csv("/content/drive/My Drive//DataAnalyst/P7_Muller_Gauthier/data/data-projet7.csv")

"""# Definitions des fonctions"""

def generate_incomes(n, pj):
    # On génère les revenus des parents (exprimés en logs) selon une loi normale.
    # La moyenne et variance n'ont aucune incidence sur le résultat final (ie. sur le caclul de la classe de revenu)
    ln_y_parent = st.norm(0,1).rvs(size=n)
    # Génération d'une réalisation du terme d'erreur epsilon
    residues = st.norm(0,1).rvs(size=n)
    return np.exp(pj*ln_y_parent + residues), np.exp(ln_y_parent)
    
def quantiles(l, nb_quantiles):
    size = len(l)
    l_sorted = l.copy()
    l_sorted = l_sorted.sort_values()
    quantiles = np.round(np.arange(1, nb_quantiles+1, nb_quantiles/size) -0.5 +1./size)
    q_dict = {a:int(b) for a,b in zip(l_sorted,quantiles)}
    return pd.Series([q_dict[e] for e in l])

def compute_quantiles(y_child, y_parents, nb_quantiles):
    y_child = pd.Series(y_child)
    y_parents = pd.Series(y_parents)
    c_i_child = quantiles(y_child, nb_quantiles)
    c_i_parent = quantiles(y_parents, nb_quantiles)
    sample = pd.concat([y_child, y_parents, c_i_child, c_i_parent], axis=1)
    sample.columns = ["y_child", "y_parents", "c_i_child","c_i_parent"]
    return sample

def distribution(counts, nb_quantiles):
    distrib = []
    total = counts["counts"].sum()
    
    if total == 0 :
        return [0] * nb_quantiles
    
    for q_p in range(1, nb_quantiles+1):
        subset = counts[counts.c_i_parent == q_p]
        if len(subset):
            nb = subset["counts"].values[0]
            distrib += [nb / total]
        else:
            distrib += [0]
    return distrib   

def conditional_distributions(sample, nb_quantiles):
    counts = sample.groupby(["c_i_child","c_i_parent"]).apply(len)
    counts = counts.reset_index()
    counts.columns = ["c_i_child","c_i_parent","counts"]
    
    mat = []
    for child_quantile in np.arange(nb_quantiles)+1:
        subset = counts[counts.c_i_child == child_quantile]
        mat += [distribution(subset, nb_quantiles)]
    return np.array(mat) 

def plot_conditional_distributions(p, cd, nb_quantiles):
    plt.figure()
    
    # La ligne suivante sert à afficher un graphique en "stack bars", sur ce modèle : https://matplotlib.org/gallery/lines_bars_and_markers/bar_stacked.html
    cumul = np.array([0] * nb_quantiles)
    
    for i, child_quantile in enumerate(cd):
        plt.bar(np.arange(nb_quantiles)+1, child_quantile, bottom=cumul, width=0.95, label = str(i+1) +"e")
        cumul = cumul + np.array(child_quantile)

    plt.axis([.5, nb_quantiles*1.3 ,0 ,1])
    plt.title("p=" + str(p))
    plt.legend()
    plt.xlabel("quantile parents")
    plt.ylabel("probabilité du quantile enfant")
    plt.show()
    
def proba_cond(c_i_parent, c_i_child, mat):
    return mat[c_i_child, c_i_parent]

def describeall(df):
    df1 = df.describe(include = 'all')
    df1.loc['dtype'] = df.dtypes
    df1.loc['size'] = len(df)
    df1.loc['% 0'] = df.isnull().mean()
    df1.loc['% NaN'] = df.isna().mean()
    ##df1.loc['sum'] = df.sum()
    return df1

def G(v):
    bins = np.linspace(0., 100., 11)
    total = float(np.sum(v))
    yvals = []
    for b in bins:
        bin_vals = v[v <= np.percentile(v, b)]
        bin_fraction = (np.sum(bin_vals) / total) * 100.0
        yvals.append(bin_fraction)
    # perfect equality area
    pe_area = np.trapz(bins, x=bins)
    # lorenz area
    lorenz_area = np.trapz(yvals, x=bins)
    gini_val = (pe_area - lorenz_area) / float(pe_area)
    return bins, yvals, gini_val
    
def smooth(x,y, box_percent=0.05,res=50,median=True):
    surface = max(x)-min(x)
    my_pas = np.arange(min(x),max(x),surface/res)
    box = surface*box_percent
    demi_box = box/2
    y_sortie = np.array([])
    x_sortie = np.array([])
    for myx in my_pas :
        temp = [y[i] for i in range(len(x)) if ((x[i]>=(myx-demi_box))and(x[i]<=(myx+demi_box)))]
        if median==True :
            temp_y = np.median(temp)
        else :
            temp_y = np.mean(temp)
        #print(temp_y)
        y_sortie = np.append(y_sortie,temp_y)
        #print(y_sortie)
        x_sortie = np.append(x_sortie,myx)
    return x_sortie, y_sortie
    
def lorenz (pays) : ## fonction qui calcul les valeurs de la courbes de  lorenz
    income = data2.loc[(data2['countryname']==pays)]['income'].values
    lorenz = np.cumsum(np.sort(income)) / income.sum()
    lorenz = np.append([0], lorenz)
    return lorenz

"""# Mission 1

##Imports des fichiers
"""

data_raw = pd.read_csv("/content/drive/My Drive//DataAnalyst/P7_Muller_Gauthier/rendu/data-projet7.csv")

"""## Analyse des données"""

## Analyse du jeu de données
data_raw.profile_report()

"""## Nettoyage et préparation du jeu de données"""

## Preparation du jeu de données
## remplacement gdpppp Fidji ==> 
data_raw.loc[data_raw['country']=='FJI','gdpppp']=10771.44
## Ajout gdpppp kosovo 2008 ==>
data_raw.loc[data_raw['country']=='XKX','gdpppp']=7949.56
## Ajout gdpppp palestine 2009 ==>
data_raw.loc[data_raw['country']=='PSE','gdpppp']=4965.37

# Calcul du quantile manquant par la moyenne des quantiles autour
income_ltu_41 = data_raw.loc[(data_raw['country'] == 'LTU') & (data_raw['quantile'] == 42) |
        (data_raw['country'] == 'LTU') & (data_raw['quantile'] == 40),'income'].mean()
income_ltu_41

## Creation ligne 41 lituanie
ltu_41 = pd.DataFrame({'country' :  ['LTU'],'year_survey' : [2008], 'quantile' : [41],'nb_quantiles' : [100],'income' : [income_ltu_41],'gdpppp'  :  [17571.0]})

## Ajout ligne au df
data_raw=data_raw.append(ltu_41,ignore_index=True)
data_raw.sort_values(by=['country', 'quantile'], inplace=True)

data_raw.tail()

## Analyse du jeu de données
prof=data_raw.profile_report()
prof.to_file(output_file='/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/analyse_data_raw.html')

"""# Mission 2

## Preparation des données
"""

population = pd.read_csv('/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/population_wb_2008.csv')

"""## Ajout de la population"""

data2=pd.merge(data_raw,population,how='left',left_on=['country'],right_on=['Country Code'])

data2[data2.isna().any(axis=1)]

## Ajout population taiwan 2008 = 23,037,000 link:https://countryeconomy.com/demography/population/taiwan?year=2008
data2.loc[data2['country']=='TWN','Population 2008']=23037000.00

## suppression et renommage des colonnes inutiles
data2.rename(columns={'Country Name':'countryname','Population 2008':'pop_2008'},inplace=True)
data2.drop(['Country Code','nb_quantiles'], axis = 1, inplace = True)

##Calcul de la population couverte par l'etude lien https://www.politologue.com/population-mondiale/ 
pop_monde_2008 = 6789771253.00
pop_2008 = data2.groupby(by=['countryname','pop_2008']).mean().reset_index()
pop_2008 = pop_2008['pop_2008'].sum()
pop = round(pop_2008/pop_monde_2008*100,2)
print("{}% de la popualtion mondiale de 2008 est couverte par l'etude".format(pop))

"""## Indice de gini"""

## Calcul de l'indice de Gini pour chaques pays ~40 secondes
tic = time.time()
list_pays=data2['countryname'].unique() ## liste de chaque pays
id=0 
list_gini=[] ## creation d'une liste vierge pour indice de gini
for Pays in list_pays:
      v=data2.loc[(data2['countryname']==Pays)]['income']
      bins, result, gini_val = G(v)
      for i in range(100) :
        list_gini.append(gini_val) ## ajoute en fin de liste l'indice
toc = time.time()
print(str(toc - tic)+" seconds")

## ajout liste pays au dataframe
data2['gini']=pd.Series(list_gini)

prof = data2.profile_report()
prof.to_file(output_file='/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/analyse_data2.html')

"""## Etude"""

## Aggregation par pays
data2_agg = data2.groupby(by=['country','countryname','pop_2008']).mean().reset_index()

## Liste des pays etudiés
liste_pays_etud = ['France','Afrique du Sud','Fédération de Russie','Nigéria','Chine','Brésil','Inde','Norvège']

"""### representation graphique """

##Représentation des revenus par quantile sur echelle logarythme
fig = plt.figure(figsize=(16, 16))
## Tracé + legende pour chaque pays
for pays in liste_pays_etud:
  x = data2.loc[(data2['countryname']==pays)]['quantile']
  y = data2.loc[(data2['countryname']==pays)]['income']

  plt.plot(x,y)
  ## nom pays au point ( 50, income[quantile == 50])
  val_text =  data2.loc[(data2['countryname']==pays) & (data2['quantile']==50)]['income']
  plt.text(55, val_text.iloc[0] , pays, fontsize = '12', rotation = 7)

## Parametre du graphique
plt.grid()
plt.yscale("log")
plt.xlabel("Income",fontsize=20)
plt.ylabel("Quantiles",fontsize=20)
plt.title('Représentation des revenus par quantile',fontsize=25)
plt.savefig('/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/Représentation des revenus par quantile.png')
plt.show()

## Representation graphique de l'indice de gini
plt.figure(figsize=(15,15))

# Tracé + legende pour chaque pays avec fonction lorenz()
for pays in liste_pays_etud:
  x = np.linspace(0,1,len(lorenz(pays)))
  y = lorenz(pays)
  plt.plot(x,y)

# Paramètre de la droite
x = [0, 1]
y = [0, 1]
plt.plot(x, y, 'red')
plt.text(0.5, 0.515, 'Indice = 1', fontsize=18, rotation = 45, color ='red')

## Parametre du graphique
plt.title('Indice de Gini par pays', fontsize=25)
plt.ylabel('proportion des revenus', fontsize=18)
plt.yticks(fontsize=15)
plt.xlabel('proportion de population', fontsize=18)
plt.xticks(fontsize=15)
plt.legend(liste_pays_etud, prop={'size':25})
plt.tight_layout()
plt.savefig('/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/Indice de gini.png')
plt.show()

"""###Evolution indice de gini

Graphiqe fait a partir de Power BI, disponible dans le dossier rendu
"""

## Prepraration des données
evol_gini = pd.read_csv('/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/worldbank/data_gini_wb.csv')

evol_gini = pd.merge(data2_agg[['country','countryname']],evol_gini,how='left',left_on=['country'],right_on=['Country Code'])

#Top 5 des pays les moins egalitaire
data2_agg[["countryname","gini"]].sort_values(by='gini',ascending=False).head(5)

## Top 5 des pays les plus egalitaire
data2_agg[["countryname","gini"]].sort_values(by='gini',ascending=False).tail(5)

##Moyenne de l'indice de gini sur les 116 pays etudié
data2_agg['gini'].mean()

## Classement de la France
data_fr = data2_agg[["countryname","gini"]].sort_values(by='gini')
data_fr.loc[(data_fr['countryname']=='France')].reset_index()

"""# Mission 3

## Modification des fichiers
"""

## Import du fichier Elasticité
elas_raw = pd.read_csv('/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/elas2018.csv')

## Creation d'un df par pays des coeff d'elasticité
elas_wb=elas_raw[['iso3','region','IGEincome']]
elas_wb=elas_wb.groupby(['iso3','region'])['IGEincome'].mean()
elas_wb=elas_wb.reset_index()

## Creation d'un df avec les valeurs du fichier Elas.txt
elas_extra = pd.DataFrame({'region': ['Latin America & Caribbean', 'Middle East & North Africa', 'Sub-Saharan Africa', 'High income','South Asia','Europe & Central Asia','East Asia & Pacific'],
                   'ige_extra': [0.66,0.66, 0.66,0.4,0.5,0.4,0.4]})

## Jointure 
elas=pd.merge(elas_wb,elas_extra,how='inner',on='region',sort = False)

## remplamcment des NaN par la valeur ige_extra
elas.IGEincome.fillna(elas.ige_extra,inplace= True)
del elas['ige_extra']

data_complet=pd.merge(data2,elas,how='left',left_on=['country'],right_on=['iso3'])

data_complet.loc[data_complet['IGEincome'].isnull()]

## Ajout des informations manquante 
data_complet.loc[data_complet['country']=='XKX','IGEincome']=0.4
data_complet.loc[data_complet['country']=='XKX','iso3']='XKX'
data_complet.loc[data_complet['country']=='XKX','countryname']='Kosovo'
data_complet.loc[data_complet['country']=='XKX','region']='Europe & Central Asia'
data_complet.loc[data_complet['country']=='SYR','IGEincome']=0.66
data_complet.loc[data_complet['country']=='SYR','iso3']='SYR'
data_complet.loc[data_complet['country']=='SYR','countryname']='Syrie'
data_complet.loc[data_complet['country']=='SYR','region']='Middle East & North Africa'

data_complet=data_complet[['countryname','quantile','gdpppp','income','IGEincome','gini']].reset_index().set_index('index')
data_complet.rename(columns={'countryname':'nom_pays','quantile':'C_i_child','IGEincome':'pj'},inplace=True)

data_complet['income']=round(data_complet['income'],2)
data_complet['gdpppp']=round(data_complet['gdpppp'],2)
data_complet['pj']=round(data_complet['pj'],3)

data_complet.dtypes

"""## Création d'un echantillon 500x"""

data_total=data_complet.reindex(np.repeat(data_complet.index.values,500)).reset_index()
data_total['C_i_parents']=0
data_total.drop(['index'],axis=1,inplace=True)

"""## Calcul des probabilité  des classes parents"""

## configuration des types de variable
data_total['C_i_child']=data_total['C_i_child'].astype(np.int8)
data_total['income']=data_total['income'].astype(np.float64)
data_total['pj']=data_total['pj'].astype(np.float64)
data_total['gdpppp']=data_total['gdpppp'].astype(np.float64)
data_total['C_i_parents']=data_total['C_i_parents'].astype(np.int8)
data_total['nom_pays']=data_total['nom_pays'].astype('category')

## Creation d'une liste avec les probabilités /!\ Prevoir ~ 10 mins /!\ 
tic = time.time()
list_pays=data_total['nom_pays'].unique() ## liste de chaque pays
id=0 
list_proba=[] ## creation d'une liste vierge pour proba ciparents
list_income_parents=[] ## creation d'une liste vierge pour les revenus des parents
for Pays in list_pays:
  pj = data_total.loc[data_total['nom_pays']==Pays]['pj'].mean()# coefficient d'élasticité du pays j
  nb_quantiles = 100      # nombre de quantiles (nombre de classes de revenu)
  n  = 500*nb_quantiles   # taille de l'échantillon
  y_child, y_parents = generate_incomes(n, pj)
  sample = compute_quantiles(y_child, y_parents, nb_quantiles)
  cd = conditional_distributions(sample, nb_quantiles)
  c_i_child = 1
  c_i_parent = 1
  for c_i_child in range(100): ## boucle pour chaque enfants
    for c_i_parent in range(100) :## boucle pour chaque parents
      #income=data_total.loc[(data_total['nom_pays']==Pays) & (data_total['C_i_child']==c_i_parent+1)]['income'].mean()
      nb_individus = int(proba_cond(c_i_parent, c_i_child, cd)*500) ## nombre d'individus concerner par cette probabilité 
      for i in range(nb_individus) :
        list_proba.append(c_i_parent+1) ## ajoute en fin de liste la probabilité 
        #list_income_parents.append(income)
        #data_total.loc[data_total.index[id],'C_i_parents']= c_i_parent+1 ## methode plus longue qui travaille directement sur le DF
        #id+= 1
  c_i_parent+= 1
toc = time.time()
print(str(toc - tic)+" seconds")

## controle
len(list_proba)

## Income moyen par pays 
tic = time.time()
list_pays=data_total['nom_pays'].unique() ## liste de chaque pays
id=0 
list_income_moy=[] ## creation d'une liste vierge pour indice de gini
for Pays in list_pays:
      v=data_total.loc[(data_total['nom_pays']==Pays)]['income'].mean()
      for i in range(50000) :
        list_income_moy.append(v) ## ajoute en fin de liste la probabilité
toc = time.time()
print(str(toc - tic)+" seconds")

## ajout des listes au dataframe
data_total['C_i_parents']=pd.Series(list_proba)
data_total['C_i_parents']=data_total['C_i_parents'].astype(np.int8)
data_total['income_moy_pays']=pd.Series(list_income_moy)
## data_total['income_parents']=pd.Series(list_income_parents)
## data_total['income_parents']=data_total['income_parents'].astype(np.float64)

data_total.describe()

#data_total.drop(['C_i_child'], axis = 1, inplace = True)
## Sauvegarde CSV
data_total.to_csv('/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/data_total.csv',index=False,sep=';')

"""# Mission 4

## Imports
"""

data_total = pd.read_csv('/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/data_total.csv',sep=';',encoding='UTF-8')

"""## Aggregation"""

## creation des colonne log
data_total['log_gdpppp'] = np.log(data_total['gdpppp'])
data_total['log_income'] = np.log(data_total['income'])
data_total['log_income_moy'] = np.log(data_total['income_moy_pays'])
data_total['log_C_i_parents'] = np.log(data_total['C_i_parents'])

## Impossible de faire les missions demandés avec 5M de lignes sur notre portables personnel. On aggrege les données 
data_agg=data_total.groupby(by=['nom_pays',"gdpppp","pj","income","gini","log_gdpppp","log_income"]).mean().reset_index()
data_agg.head(101)

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

def draw_hist(x, **kws):
    plt.hist(x[~np.isnan(x)])

def corr_func(x, y, **kws):
    mask = ~np.logical_or(np.isnan(x), np.isnan(y))
    x, y = x[mask], y[mask]
    r, _ = stats.pearsonr(x, y)
    ax = plt.gca()
    ax.annotate("r = {:.3f}".format(r),
               xy=(.2, .5), 
               xycoords=ax.transAxes,
               size=16)

def pairplot(df):
    g = sns.PairGrid(df, height=1.6, dropna=False)
    g.map_diag(draw_hist)
    g.map_upper(sns.regplot, scatter_kws={"s": 8}, line_kws={"color":  "r"})
    g.map_lower(corr_func)

pairplot(data_agg[['nom_pays','gdpppp','income','gini','C_i_parents','income_moy_pays']])

data_agg2=data_agg[['nom_pays','gdpppp','income','gini','C_i_parents','income_moy_pays']]
prof = data_agg.profile_report()
prof.to_file(output_file='/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/analyse_data_agg.html')

##Controle 
len(data_agg)

"""## Anova"""

## Anova income / pays
fit = statsmodels.formula.api.ols('income ~ nom_pays', data_agg).fit()
table = statsmodels.api.stats.anova_lm(fit)
print(fit.summary().tables[0])
print(table)

## Test de normalité
st.shapiro(fit.resid)

## QQ plots 
res = fit.resid_pearson
fig = sm.qqplot(res)
plt.show()

"""##  1. Regression Gini + gdpppp"""

## regression multiple
X = data_agg[['gini',"income_moy_pays"]]
X = sm.add_constant(X)
y = data_agg['income']
model = sm.OLS(y,X) ## statsmodels.formula.api.ols('income ~ gini + gdpppp', data=data_agg)
results = model.fit()
res = results.resid ## residues standard
res_pearson = results.resid_pearson ## residues standard de pearson
print(results.summary())

## Autre methode de regression
model2=statsmodels.formula.api.ols('income ~ gini + income_moy_pays', data=data_agg)
results_anov = model2.fit()
# Anova sur les variables de notre régression linéaire
anova = sm.stats.anova_lm(results_anov)
anova

# Calcul des r2 pour chaque variables
rsqrt_gini = anova.sum_sq[anova.index[0]] / anova['sum_sq'].sum()
rsqrt_gdppp = anova.sum_sq[anova.index[1]] / anova['sum_sq'].sum()

## Conclusion
print('{}% de la variabilité du modele est expliqué par les variables {}'.format(round(results.rsquared *100,2),X.columns.values.tolist()))
print('La variable {} explique {}% de la variance de income '.format(anova.index[0],round(rsqrt_gini*100,2)))
print('La variable {} explique {}% de la variance de income '.format(anova.index[1],round(rsqrt_gdppp*100,2)))

"""### **Performance du modele**"""

## Parametre 
alpha = 0.05
n = X.shape[0]
p = len(X.columns.values.tolist())

"""#### Calculez les leviers

On peut calculer les leviers comme ceci, en sachant que le seuil des leviers est de $2∗\frac{p}{n}$.
"""

## Creation d'une colonne levier 
data_agg['levier'] = results.get_influence().hat_matrix_diag
## Calcul du seuil
seuil_levier = 2*p/n
## Creation des variables 
atyp_levier = data_agg.loc[data_agg['levier'] > seuil_levier]
atyp_levier_pays = atyp_levier.groupby(['nom_pays']).mean().reset_index()
## Affichage du nombre d'individu
print('{} individu ont des leviers supérieurs aux seuil de {}'.format(len(atyp_levier),round(seuil_levier,6)))
## Affichage du nombre de pays concerné 
print('{} pays ont des leviers supérieurs aux seuil de {}'.format(len(atyp_levier_pays),round(seuil_levier,6)))

atyp_levier_pays

## Graphique
plt.figure(figsize=(18,8))
plt.bar(atyp_levier_pays['nom_pays'],atyp_levier_pays['levier'])
plt.xticks(np.arange(0, len(atyp_levier_pays), step=1))
plt.xlabel('Pays')
plt.ylabel('Leviers')
plt.plot([0, len(atyp_levier_pays)], [seuil_levier, seuil_levier], color='r')
plt.show()

"""#### Calculez les résidus studentisés"""

## Création d'une colonne des residus studentisés
data_agg['rstudent'] = results.get_influence().resid_studentized_internal
## Seuil = loi de student a n-p-1 degrés de liberté
seuil_rstudent = t.ppf(1-alpha/2,n-p-1)
## Création des variables
atyp_stud = data_agg.loc[np.abs(data_agg['rstudent']) > seuil_rstudent]
atyp_stud_pays = atyp_stud.groupby(['nom_pays']).mean().reset_index()
## Affichage du nombre d'individu
print('{} individu ont des leviers supérieurs aux seuil de {}'.format(len(atyp_stud),round(seuil_rstudent,6)))
## Affichage du nombre de pays concerné 
print('{} pays ont des leviers supérieurs aux seuil de {}'.format(len(atyp_stud_pays),round(seuil_rstudent,6)))

## Affichage
atyp_stud_pays

## Graphique 
plt.figure(figsize=(18,8))
plt.bar(atyp_stud_pays['nom_pays'], atyp_stud_pays['rstudent'])
plt.xticks(np.arange(0, len(atyp_stud_pays), step=1),rotation=70)
plt.xlabel('Observation')
plt.ylabel('Résidus studentisés')
plt.plot([0, len(atyp_stud_pays)], [seuil_rstudent, seuil_rstudent], color='r')
plt.plot([0, len(atyp_stud_pays)], [-seuil_rstudent, -seuil_rstudent], color='r')
plt.show()

"""#### Déterminez la distance de Cook

Pour trouver la distance de Cook, nous exécutons ceci :
"""

## Calcul des distances de cooks
influence = results.get_influence().summary_frame()
data_agg['dcooks']=influence['cooks_d']
## Seuil de cook
seuil_dcook = 4/(n-p)
## Variable contenant les pays ciblé
atyp_cook = data_agg.loc[np.abs(data_agg['dcooks']) > seuil_dcook]
atyp_cook_pays = atyp_cook.groupby(['nom_pays']).mean().reset_index()

"""Le seuil de la distance de Cook est de n-p.

On peut détecter les observations influentes comme ceci :
"""

plt.figure(figsize=(18,8))
plt.bar(atyp_cook_pays['nom_pays'], atyp_cook_pays['dcooks'])
plt.xticks(np.arange(0, len(atyp_cook_pays), step=1),rotation = 70)
plt.xlabel('Observation')
plt.ylabel('Leviers')
plt.plot([0, len(atyp_cook_pays)], [seuil_dcook, seuil_dcook], color='r')
plt.show()

"""#### Individus atypiques et influents"""

## Variables contnant les pays
atyp_cook = data_agg.loc[np.abs(data_agg['dcooks']) > seuil_dcook]
atyp_stud = data_agg.loc[np.abs(data_agg['rstudent']) > seuil_rstudent]
atyp_levier = data_agg.loc[data_agg['levier'] > seuil_levier]

## Pays atypique et influents
data_atyp = data_agg.loc[(np.abs(data_agg['dcooks']) > seuil_dcook) & (np.abs(data_agg['rstudent']) > seuil_rstudent) & (data_agg['levier'] > seuil_levier)]

data_atyp_agg = data_atyp.groupby(['nom_pays']).mean().reset_index()

data_atyp_agg

## Influence plot
from statsmodels.graphics.regressionplots import *
fig, ax = plt.subplots(figsize=(12,8))
fig =influence_plot(results, ax=ax)
plt.savefig('/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/reg_influenceplot.png')

data_agg.shape

pays = list(data_atyp_agg['nom_pays'])
data_perf = data_agg[~data_agg.nom_pays.isin(pays)]
data_perf.shape

"""On ne retire des points qu'après avoir vérifié qu'ils sont effectivement atypiques, voire aberrants, au vu du modèle estimé.

### Conditions de validité du modele

#### Vérifier la colinéarité des variables

Une autre chose à vérifier est l'éventuelle colinéarité approchée des variables :
"""

variables = results.model.exog
[variance_inflation_factor(variables, i) for i in np.arange(1,variables.shape[1])]

"""Ici, tous les coefficients sont inférieurs à 10, il n'y a donc pas de problème de colinéarité."""

#matrice des corrélations avec scipy
mc = np.corrcoef(X,rowvar = 0)
print(mc)

#règle de Klein
mc2 = mc**2
print(mc2)

#critère VIF
vif = np.linalg.inv(mc)
print(vif)

#etude de la correlation
matrice_corr = data_agg.corr().round(1)
sns.heatmap(data=matrice_corr, annot=True)

"""
#### Testez l’homoscédasticité

On peut également tester l’homoscédasticité (c'est-à-dire la constance de la variance) des résidus :"""

_, pval, __, f_pval = statsmodels.stats.diagnostic.het_breuschpagan(results.resid, variables)
print('p value test Breusch Pagan:', pval)

"""La p-valeur ici est inférieure à  5 %, on  rejette  l'hypothèse  H0  selon laquelle les variances sont constantes (l'hypothèse d’homoscédasticité)."""

##sqrt_residues_std = np.sqrt(abs(residues_std)) # ne fonctionne pas si on ne met pas abs
sqrt_residues_std = results.get_influence().resid_studentized_internal
fitted_values =  results.predict(X)
from matplotlib.pyplot import plot, scatter, show, xlabel, ylabel
scatter(fitted_values,sqrt_residues_std)
xlabel("Fitted values")
ylabel("Racine carrée des résidus standardisés")
# en reprenant la fonction smooth() plus haut
xs , ys = smooth(fitted_values,sqrt_residues_std,box_percent=0.25,res=30)
plot(xs,ys,"-r")
show()

"""####1) Les individus sont indépendantes

"""

from statsmodels.graphics.tsaplots import plot_acf
plot_acf(res)

"""#### 2) Les résidus suivent une distribution normale ( Shapiro )"""

## Test de shapiro
st.shapiro(res_pearson)

## Test de kolmogorov
ks_2samp(results.resid,list(np.random.normal(np.mean(res), np.std(res), len(data_agg))))

##Visualisation
plt.hist(res,100)
plt.ylabel('Count')
plt.xlabel('Normalized residuals')
plt.xticks(fontsize=8)
plt.show()

## QQ plot
residues_std = results.get_influence().resid_studentized_internal
fig = sm.qqplot(residues_std, st.t, fit=True,line='45')
plt.title("QQ plot")
plt.show()

"""##  2. Regression Gini + gdpppp LOG"""

## regression multiple
X = data_agg[['gini',"log_income_moy"]]
X = sm.add_constant(X)
y = data_agg['log_income']
model = sm.OLS(y,X) ## statsmodels.formula.api.ols('income ~ gini + gdpppp', data=data_agg)
results = model.fit()
res = results.resid ## residues standard
res_pearson = results.resid_pearson ## residues standard de pearson
print(results.summary())

## Autre methode de regression
model2=statsmodels.formula.api.ols('log_income ~ gini + log_income_moy', data=data_agg)
results_anov = model2.fit()
# Anova sur les variables de notre régression linéaire
anova = sm.stats.anova_lm(results_anov)
anova

# Calcul des r2 pour chaque variables
rsqrt_gini = anova.sum_sq[anova.index[0]] / anova['sum_sq'].sum()
rsqrt_gdppp = anova.sum_sq[anova.index[1]] / anova['sum_sq'].sum()

## Conclusion
print('{}% de la variabilité du modele est expliqué par les variables {}'.format(round(results.rsquared *100,2),X.columns.values.tolist()))
print('La variable {} explique {}% de la variance de income '.format(anova.index[0],round(rsqrt_gini*100,2)))
print('La variable {} explique {}% de la variance de income '.format(anova.index[1],round(rsqrt_gdppp*100,2)))

"""### **Performance du modele**"""

## Parametre 
alpha = 0.05
n = X.shape[0]
p = len(X.columns.values.tolist())

"""#### Calculez les leviers

On peut calculer les leviers comme ceci, en sachant que le seuil des leviers est de $2∗\frac{p}{n}$.
"""

## Creation d'une colonne levier 
data_agg['levier'] = results.get_influence().hat_matrix_diag
## Calcul du seuil
seuil_levier = 2*p/n
## Creation des variables 
atyp_levier = data_agg.loc[data_agg['levier'] > seuil_levier]
atyp_levier_pays = atyp_levier.groupby(['nom_pays']).mean().reset_index()
## Affichage du nombre d'individu
print('{} individu ont des leviers supérieurs aux seuil de {}'.format(len(atyp_levier),round(seuil_levier,6)))
## Affichage du nombre de pays concerné 
print('{} pays ont des leviers supérieurs aux seuil de {}'.format(len(atyp_levier_pays),round(seuil_levier,6)))

atyp_levier_pays

## Graphique
plt.figure(figsize=(18,8))
plt.bar(atyp_levier_pays['nom_pays'],atyp_levier_pays['levier'])
plt.xticks(np.arange(0, len(atyp_levier_pays), step=1))
plt.xlabel('Pays')
plt.ylabel('Leviers')
plt.plot([0, len(atyp_levier_pays)], [seuil_levier, seuil_levier], color='r')
plt.show()

"""#### Calculez les résidus studentisés"""

## Création d'une colonne des residus studentisés
data_agg['rstudent'] = results.get_influence().resid_studentized_internal
## Seuil = loi de student a n-p-1 degrés de liberté
seuil_rstudent = t.ppf(1-alpha/2,n-p-1)
## Création des variables
atyp_stud = data_agg.loc[np.abs(data_agg['rstudent']) > seuil_rstudent]
atyp_stud_pays = atyp_stud.groupby(['nom_pays']).mean().reset_index()
## Affichage du nombre d'individu
print('{} individu ont des leviers supérieurs aux seuil de {}'.format(len(atyp_stud),round(seuil_rstudent,6)))
## Affichage du nombre de pays concerné 
print('{} pays ont des leviers supérieurs aux seuil de {}'.format(len(atyp_stud_pays),round(seuil_rstudent,6)))

## Affichage
atyp_stud_pays

## Graphique 
plt.figure(figsize=(18,8))
plt.bar(atyp_stud_pays['nom_pays'], atyp_stud_pays['rstudent'])
plt.xticks(np.arange(0, len(atyp_stud_pays), step=1),rotation=70)
plt.xlabel('Observation')
plt.ylabel('Résidus studentisés')
plt.plot([0, len(atyp_stud_pays)], [seuil_rstudent, seuil_rstudent], color='r')
plt.plot([0, len(atyp_stud_pays)], [-seuil_rstudent, -seuil_rstudent], color='r')
plt.show()

"""#### Déterminez la distance de Cook

Pour trouver la distance de Cook, nous exécutons ceci :
"""

## Calcul des distances de cooks
influence = results.get_influence().summary_frame()
data_agg['dcooks']=influence['cooks_d']
## Seuil de cook
seuil_dcook = 4/(n-p)
## Variable contenant les pays ciblé
atyp_cook = data_agg.loc[np.abs(data_agg['dcooks']) > seuil_dcook]
atyp_cook_pays = atyp_cook.groupby(['nom_pays']).mean().reset_index()

"""Le seuil de la distance de Cook est de n-p.

On peut détecter les observations influentes comme ceci :
"""

plt.figure(figsize=(18,8))
plt.bar(atyp_cook_pays['nom_pays'], atyp_cook_pays['dcooks'])
plt.xticks(np.arange(0, len(atyp_cook_pays), step=1),rotation = 70)
plt.xlabel('Observation')
plt.ylabel('Leviers')
plt.plot([0, len(atyp_cook_pays)], [seuil_dcook, seuil_dcook], color='r')
plt.show()

"""#### Individus atypiques et influents"""

## Variables contnant les pays
atyp_cook = data_agg.loc[np.abs(data_agg['dcooks']) > seuil_dcook]
atyp_stud = data_agg.loc[np.abs(data_agg['rstudent']) > seuil_rstudent]
atyp_levier = data_agg.loc[data_agg['levier'] > seuil_levier]

## Pays atypique et influents
data_atyp = data_agg.loc[(np.abs(data_agg['dcooks']) > seuil_dcook) & (np.abs(data_agg['rstudent']) > seuil_rstudent) & (data_agg['levier'] > seuil_levier)]

data_atyp_agg = data_atyp.groupby(['nom_pays']).mean().reset_index()

data_atyp_agg

## Influence plot
from statsmodels.graphics.regressionplots import *
fig, ax = plt.subplots(figsize=(12,8))
fig =influence_plot(results, ax=ax)
plt.savefig('/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/reg_influenceplot.png')

data_agg.shape

pays = list(data_atyp_agg['nom_pays'])
data_perf = data_agg[~data_agg.nom_pays.isin(pays)]
data_perf.shape

"""On ne retire des points qu'après avoir vérifié qu'ils sont effectivement atypiques, voire aberrants, au vu du modèle estimé.

### Conditions de validité du modele

#### Vérifier la colinéarité des variables

Une autre chose à vérifier est l'éventuelle colinéarité approchée des variables :
"""

variables = results.model.exog
[variance_inflation_factor(variables, i) for i in np.arange(1,variables.shape[1])]

"""Ici, tous les coefficients sont inférieurs à 10, il n'y a donc pas de problème de colinéarité."""

#matrice des corrélations avec scipy
mc = np.corrcoef(X,rowvar = 0)
print(mc)

#règle de Klein
mc2 = mc**2
print(mc2)

#critère VIF
vif = np.linalg.inv(mc)
print(vif)

#etude de la correlation
matrice_corr = data_agg.corr().round(1)
sns.heatmap(data=matrice_corr, annot=True)

"""
#### Testez l’homoscédasticité

On peut également tester l’homoscédasticité (c'est-à-dire la constance de la variance) des résidus :"""

_, pval, __, f_pval = statsmodels.stats.diagnostic.het_breuschpagan(results.resid, variables)
print('p value test Breusch Pagan:', pval)

"""La p-valeur ici est inférieure à  5 %, on  rejette  l'hypothèse  H0  selon laquelle les variances sont constantes (l'hypothèse d’homoscédasticité)."""

##sqrt_residues_std = np.sqrt(abs(residues_std)) # ne fonctionne pas si on ne met pas abs
sqrt_residues_std = results.get_influence().resid_studentized_internal
fitted_values =  results.predict(X)
from matplotlib.pyplot import plot, scatter, show, xlabel, ylabel
scatter(fitted_values,sqrt_residues_std)
xlabel("Fitted values")
ylabel("Racine carrée des résidus standardisés")
# en reprenant la fonction smooth() plus haut
xs , ys = smooth(fitted_values,sqrt_residues_std,box_percent=0.25,res=30)
plot(xs,ys,"-r")
show()

"""####1) Les individus sont indépendantes

"""

from statsmodels.graphics.tsaplots import plot_acf
plot_acf(res)

"""#### 2) Les résidus suivent une distribution normale ( Shapiro )"""

## Test de shapiro
st.shapiro(res_pearson)

## Test de kolmogorov
ks_2samp(results.resid,list(np.random.normal(np.mean(res), np.std(res), len(data_agg))))

##Visualisation
plt.hist(res,100)
plt.ylabel('Count')
plt.xlabel('Normalized residuals')
plt.xticks(fontsize=8)
plt.show()

## QQ plot
residues_std = results.get_influence().resid_studentized_internal
fig = sm.qqplot(residues_std, st.t, fit=True,line='45')
plt.title("QQ plot")
plt.show()

"""##  3. Regression Gini + gdpppp + C_i_parents"""

## regression multiple
X = data_agg[['gini',"income_moy_pays",'C_i_parents']]
X = sm.add_constant(X)
y = data_agg['income']
model = sm.OLS(y,X) ## statsmodels.formula.api.ols('income ~ gini + gdpppp', data=data_agg)
results = model.fit()
res = results.resid ## residues standard
res_pearson = results.resid_pearson ## residues standard de pearson
print(results.summary())

## Autre methode de regression
model2=statsmodels.formula.api.ols('income ~ gini + income_moy_pays + C_i_parents', data=data_agg)
results_anov = model2.fit()
# Anova sur les variables de notre régression linéaire
anova = sm.stats.anova_lm(results_anov)
anova

# Calcul des r2 pour chaque variables
rsqrt_gini = anova.sum_sq[anova.index[0]] / anova['sum_sq'].sum()
rsqrt_gdppp = anova.sum_sq[anova.index[1]] / anova['sum_sq'].sum()
rsqrt_C_i_parents = anova.sum_sq[anova.index[2]] / anova['sum_sq'].sum()

## Conclusion
print('{}% de la variabilité du modele est expliqué par les variables {}'.format(round(results.rsquared *100,2),X.columns.values.tolist()))
print('La variable {} explique {}% de la variance de income '.format(anova.index[0],round(rsqrt_gini*100,2)))
print('La variable {} explique {}% de la variance de income '.format(anova.index[1],round(rsqrt_gdppp*100,2)))
print('La variable {} explique {}% de la variance de income '.format(anova.index[2],round(rsqrt_C_i_parents*100,2)))

"""### **Performance du modele**"""

## Parametre 
alpha = 0.05
n = X.shape[0]
p = len(X.columns.values.tolist())

"""#### Calculez les leviers

On peut calculer les leviers comme ceci, en sachant que le seuil des leviers est de $2∗\frac{p}{n}$.
"""

## Creation d'une colonne levier 
data_agg['levier'] = results.get_influence().hat_matrix_diag
## Calcul du seuil
seuil_levier = 2*p/n
## Creation des variables 
atyp_levier = data_agg.loc[data_agg['levier'] > seuil_levier]
atyp_levier_pays = atyp_levier.groupby(['nom_pays']).mean().reset_index()
## Affichage du nombre d'individu
print('{} individu ont des leviers supérieurs aux seuil de {}'.format(len(atyp_levier),round(seuil_levier,6)))
## Affichage du nombre de pays concerné 
print('{} pays ont des leviers supérieurs aux seuil de {}'.format(len(atyp_levier_pays),round(seuil_levier,6)))

atyp_levier_pays

## Graphique
plt.figure(figsize=(18,8))
plt.bar(atyp_levier_pays['nom_pays'],atyp_levier_pays['levier'])
plt.xticks(np.arange(0, len(atyp_levier_pays), step=1))
plt.xlabel('Pays')
plt.ylabel('Leviers')
plt.plot([0, len(atyp_levier_pays)], [seuil_levier, seuil_levier], color='r')
plt.show()

"""#### Calculez les résidus studentisés"""

## Création d'une colonne des residus studentisés
data_agg['rstudent'] = results.get_influence().resid_studentized_internal
## Seuil = loi de student a n-p-1 degrés de liberté
seuil_rstudent = t.ppf(1-alpha/2,n-p-1)
## Création des variables
atyp_stud = data_agg.loc[np.abs(data_agg['rstudent']) > seuil_rstudent]
atyp_stud_pays = atyp_stud.groupby(['nom_pays']).mean().reset_index()
## Affichage du nombre d'individu
print('{} individu ont des leviers supérieurs aux seuil de {}'.format(len(atyp_stud),round(seuil_rstudent,6)))
## Affichage du nombre de pays concerné 
print('{} pays ont des leviers supérieurs aux seuil de {}'.format(len(atyp_stud_pays),round(seuil_rstudent,6)))

## Affichage
atyp_stud_pays

## Graphique 
plt.figure(figsize=(18,8))
plt.bar(atyp_stud_pays['nom_pays'], atyp_stud_pays['rstudent'])
plt.xticks(np.arange(0, len(atyp_stud_pays), step=1),rotation=70)
plt.xlabel('Observation')
plt.ylabel('Résidus studentisés')
plt.plot([0, len(atyp_stud_pays)], [seuil_rstudent, seuil_rstudent], color='r')
plt.plot([0, len(atyp_stud_pays)], [-seuil_rstudent, -seuil_rstudent], color='r')
plt.show()

"""#### Déterminez la distance de Cook

Pour trouver la distance de Cook, nous exécutons ceci :
"""

## Calcul des distances de cooks
influence = results.get_influence().summary_frame()
data_agg['dcooks']=influence['cooks_d']
## Seuil de cook
seuil_dcook = 4/(n-p)
## Variable contenant les pays ciblé
atyp_cook = data_agg.loc[np.abs(data_agg['dcooks']) > seuil_dcook]
atyp_cook_pays = atyp_cook.groupby(['nom_pays']).mean().reset_index()

"""#### Individus atypiques et influents"""

## Variables contnant les pays
atyp_cook = data_agg.loc[np.abs(data_agg['dcooks']) > seuil_dcook]
atyp_stud = data_agg.loc[np.abs(data_agg['rstudent']) > seuil_rstudent]
atyp_levier = data_agg.loc[data_agg['levier'] > seuil_levier]

## Pays atypique et influents
data_atyp = data_agg.loc[(np.abs(data_agg['dcooks']) > seuil_dcook) & (np.abs(data_agg['rstudent']) > seuil_rstudent) & (data_agg['levier'] > seuil_levier)]

data_atyp_agg = data_atyp.groupby(['nom_pays']).mean().reset_index()

data_atyp_agg

## Influence plot
from statsmodels.graphics.regressionplots import *
fig, ax = plt.subplots(figsize=(12,8))
fig =influence_plot(results, ax=ax)
plt.savefig('/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/reg_influenceplot.png')

data_agg.shape

pays = list(data_atyp_agg['nom_pays'])
data_perf = data_agg[~data_agg.nom_pays.isin(pays)]
data_perf.shape

"""On ne retire des points qu'après avoir vérifié qu'ils sont effectivement atypiques, voire aberrants, au vu du modèle estimé.

"""

influence = results.get_influence().summary_frame()
data_agg['dcooks']=influence['cooks_d']
seuil_dcook = 4/(n-p)

atyp_cook = data_agg.loc[np.abs(data_agg['dcooks']) > seuil_dcook]
atyp_cook_pays = atyp_cook.groupby(['nom_pays']).mean().reset_index()

"""Le seuil de la distance de Cook est de n-p.

On peut détecter les observations influentes comme ceci :
"""

plt.figure(figsize=(18,8))
plt.bar(atyp_cook_pays['nom_pays'], atyp_cook_pays['dcooks'])
plt.xticks(np.arange(0, len(atyp_cook_pays), step=1),rotation = 70)
plt.xlabel('Observation')
plt.ylabel('Leviers')
plt.plot([0, len(atyp_cook_pays)], [seuil_dcook, seuil_dcook], color='r')
plt.show()

"""### Conditions de validité du modele

#### Vérifier la colinéarité des variables

Une autre chose à vérifier est l'éventuelle colinéarité approchée des variables :
"""

variables = results.model.exog
[variance_inflation_factor(variables, i) for i in np.arange(1,variables.shape[1])]

"""Ici, tous les coefficients sont inférieurs à 10, il n'y a donc pas de problème de colinéarité."""

#matrice des corrélations avec scipy
mc = np.corrcoef(X,rowvar = 0)
print(mc)

#règle de Klein
mc2 = mc**2
print(mc2)

#critère VIF
vif = np.linalg.inv(mc)
print(vif)

#etude de la correlation
matrice_corr = data_agg.corr().round(1)
sns.heatmap(data=matrice_corr, annot=True)

"""
#### Testez l’homoscédasticité

On peut également tester l’homoscédasticité (c'est-à-dire la constance de la variance) des résidus :"""

_, pval, __, f_pval = statsmodels.stats.diagnostic.het_breuschpagan(results.resid, variables)
print('p value test Breusch Pagan:', pval)

"""La p-valeur ici est inférieure à  5 %, on  rejette  l'hypothèse  H0  selon laquelle les variances sont constantes (l'hypothèse d’homoscédasticité)."""

##sqrt_residues_std = np.sqrt(abs(residues_std)) # ne fonctionne pas si on ne met pas abs
sqrt_residues_std = results.get_influence().resid_studentized_internal
fitted_values =  results.predict(X)
from matplotlib.pyplot import plot, scatter, show, xlabel, ylabel
scatter(fitted_values,sqrt_residues_std)
xlabel("Fitted values")
ylabel("Racine carrée des résidus standardisés")
# en reprenant la fonction smooth() plus haut
xs , ys = smooth(fitted_values,sqrt_residues_std,box_percent=0.25,res=30)
plot(xs,ys,"-r")
show()

"""####1) Les individus sont indépendantes

"""

from statsmodels.graphics.tsaplots import plot_acf
plot_acf(res)

"""#### 2) Les résidus suivent une distribution normale ( Shapiro )"""

## Test de shapiro
st.shapiro(res_pearson)

## Test de kolmogorov
ks_2samp(results.resid,list(np.random.normal(np.mean(res), np.std(res), len(data_agg))))

##Visualisation
plt.hist(res,100)
plt.ylabel('Count')
plt.xlabel('Normalized residuals')
plt.xticks(fontsize=8)
plt.show()

## QQ plot
residues_std = results.get_influence().resid_studentized_internal
fig = sm.qqplot(residues_std, st.t, fit=True,line='45')
plt.title("QQ plot")
plt.show()

"""##  4. Regression Gini + gdpppp+ C_i_parents LOG (la plus pertinente)"""

## regression multiple
X = data_agg[['gini',"log_income_moy",'C_i_parents']]
X = sm.add_constant(X)
y = data_agg['log_income']
model = sm.OLS(y,X) ## statsmodels.formula.api.ols('income ~ gini + gdpppp', data=data_agg)
results = model.fit()
res = results.resid ## residues standard
res_pearson = results.resid_pearson ## residues standard de pearson
print(results.summary())

## Autre methode de regression
model2=statsmodels.formula.api.ols('log_income ~ gini + log_income_moy  + C_i_parents', data=data_agg)
results_anov = model2.fit()
# Anova sur les variables de notre régression linéaire
anova = sm.stats.anova_lm(results_anov)
anova

# Calcul des r2 pour chaque variables
rsqrt_gini = anova.sum_sq[anova.index[0]] / anova['sum_sq'].sum()
rsqrt_gdppp = anova.sum_sq[anova.index[1]] / anova['sum_sq'].sum()   
rsqrt_C_i_parents = anova.sum_sq[anova.index[2]] / anova['sum_sq'].sum()

## Conclusion
print('{}% de la variabilité du modele est expliqué par les variables {}'.format(round(results.rsquared *100,2),X.columns.values.tolist()))
print('La variable {} explique {}% de la variance de income '.format(anova.index[0],round(rsqrt_gini*100,2)))
print('La variable {} explique {}% de la variance de income '.format(anova.index[1],round(rsqrt_gdppp*100,2)))
print('La variable {} explique {}% de la variance de income '.format(anova.index[2],round(rsqrt_C_i_parents*100,2)))

"""### **Performance du modele**"""

## Parametre 
alpha = 0.05
n = X.shape[0]
p = len(X.columns.values.tolist())

"""#### Calculez les leviers

On peut calculer les leviers comme ceci, en sachant que le seuil des leviers est de $2∗\frac{p}{n}$.
"""

## Creation d'une colonne levier 
data_agg['levier'] = results.get_influence().hat_matrix_diag
## Calcul du seuil
seuil_levier = 2*p/n
## Creation des variables 
atyp_levier = data_agg.loc[data_agg['levier'] > seuil_levier]
atyp_levier_pays = atyp_levier.groupby(['nom_pays']).mean().reset_index()
## Affichage du nombre d'individu
print('{} individu ont des leviers supérieurs aux seuil de {}'.format(len(atyp_levier),round(seuil_levier,6)))
## Affichage du nombre de pays concerné 
print('{} pays ont des leviers supérieurs aux seuil de {}'.format(len(atyp_levier_pays),round(seuil_levier,6)))

atyp_levier_pays.sort_values(by='levier',ascending=False)

## Graphique
plt.figure(figsize=(18,8))
plt.bar(atyp_levier_pays['nom_pays'],atyp_levier_pays['levier'])
plt.xticks(np.arange(0, len(atyp_levier_pays), step=1),rotation=70)
plt.xlabel('Pays')
plt.ylabel('Leviers')
plt.plot([0, len(atyp_levier_pays)], [seuil_levier, seuil_levier], color='r')
plt.savefig('/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/reg_leviers.png')
plt.show()

"""#### Calculez les résidus studentisés"""

## Création d'une colonne des residus studentisés
data_agg['rstudent'] = results.get_influence().resid_studentized_internal
## Seuil = loi de student a n-p-1 degrés de liberté
seuil_rstudent = t.ppf(1-alpha/2,n-p-1)
## Création des variables
atyp_stud = data_agg.loc[np.abs(data_agg['rstudent']) > seuil_rstudent]
atyp_stud_pays = atyp_stud.groupby(['nom_pays']).mean().reset_index()
## Affichage du nombre d'individu
print('{} individu ont des résidus supérieurs aux seuil de {}'.format(len(atyp_stud),round(seuil_rstudent,6)))
## Affichage du nombre de pays concerné 
print('{} pays ont des résidus supérieurs aux seuil de {}'.format(len(atyp_stud_pays),round(seuil_rstudent,6)))

## Affichage
atyp_stud_pays

## Graphique 
plt.figure(figsize=(18,8))
plt.bar(atyp_stud_pays['nom_pays'], atyp_stud_pays['rstudent'])
plt.xticks(np.arange(0, len(atyp_stud_pays), step=1),rotation=70)
plt.xlabel('Observation')
plt.ylabel('Résidus studentisés')
plt.plot([0, len(atyp_stud_pays)], [seuil_rstudent, seuil_rstudent], color='r')
plt.plot([0, len(atyp_stud_pays)], [-seuil_rstudent, -seuil_rstudent], color='r')
plt.savefig('/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/reg_residus_student.png')
plt.show()

"""#### Déterminez la distance de Cook

Pour trouver la distance de Cook, nous exécutons ceci :
"""

## Calcul des distances de cooks
influence = results.get_influence().summary_frame()
data_agg['dcooks']=influence['cooks_d']
## Seuil de cook
seuil_dcook = 4/(n-p)
## Variable contenant les pays ciblé
atyp_cook = data_agg.loc[np.abs(data_agg['dcooks']) > seuil_dcook]
atyp_cook_pays = atyp_cook.groupby(['nom_pays']).mean().reset_index()

"""Le seuil de la distance de Cook est de n-p.

On peut détecter les observations influentes comme ceci :
"""

plt.figure(figsize=(18,8))
plt.bar(atyp_cook_pays['nom_pays'], atyp_cook_pays['dcooks'])
plt.xticks(np.arange(0, len(atyp_cook_pays), step=1),rotation = 70)
plt.xlabel('Observation')
plt.ylabel('Leviers')
plt.plot([0, len(atyp_cook_pays)], [seuil_dcook, seuil_dcook], color='r')
plt.savefig('/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/Représentation des revenus par quantile.png')
plt.show()
## Affichage du nombre d'individu
print('{} individu ont des distances supérieurs aux seuil de {}'.format(len(atyp_cook),round(seuil_dcook,6)))
## Affichage du nombre de pays concerné 
print('{} pays ont des distances supérieurs aux seuil de {}'.format(len(atyp_cook_pays),round(seuil_dcook,6)))

"""#### Individus atypiques et influents"""

## Variables contnant les pays
atyp_cook = data_agg.loc[np.abs(data_agg['dcooks']) > seuil_dcook]
atyp_stud = data_agg.loc[np.abs(data_agg['rstudent']) > seuil_rstudent]
atyp_levier = data_agg.loc[data_agg['levier'] > seuil_levier]

## Pays atypique et influents
data_atyp = data_agg.loc[(np.abs(data_agg['dcooks']) > seuil_dcook) & (np.abs(data_agg['rstudent']) > seuil_rstudent) & (data_agg['levier'] > seuil_levier)]

data_atyp_agg = data_atyp.groupby(['nom_pays']).mean().reset_index()

data_atyp_agg

## Influence plot
from statsmodels.graphics.regressionplots import *
fig, ax = plt.subplots(figsize=(12,8))
fig =influence_plot(results, ax=ax)
plt.savefig('/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/reg_influenceplot.png')

data_agg.shape

pays = list(data_atyp_agg['nom_pays'])
data_perf = data_agg[~data_agg.nom_pays.isin(pays)]
data_perf.shape

"""On ne retire des points qu'après avoir vérifié qu'ils sont effectivement atypiques, voire aberrants, au vu du modèle estimé.

### Conditions de validité du modele

#### Vérifier la colinéarité des variables

Une autre chose à vérifier est l'éventuelle colinéarité approchée des variables :
"""

variables = results.model.exog
[variance_inflation_factor(variables, i) for i in np.arange(1,variables.shape[1])]

"""Ici, tous les coefficients sont inférieurs à 10, il n'y a donc pas de problème de colinéarité."""

#matrice des corrélations avec scipy
mc = np.corrcoef(X,rowvar = 0)
print(mc)

#règle de Klein
mc2 = mc**2
print(mc2)

#critère VIF
vif = np.linalg.inv(mc)
print(vif)

"""
#### Testez l’homoscédasticité

On peut également tester l’homoscédasticité (c'est-à-dire la constance de la variance) des résidus :"""

_, pval, __, f_pval = statsmodels.stats.diagnostic.het_breuschpagan(results.resid, variables)
print('p value test Breusch Pagan:', pval)

"""La p-valeur ici est inférieure à  5 %, on  rejette  l'hypothèse  H0  selon laquelle les variances sont constantes (l'hypothèse d’homoscédasticité)."""

##sqrt_residues_std = np.sqrt(abs(residues_std)) # ne fonctionne pas si on ne met pas abs
sqrt_residues_std = results.get_influence().resid_studentized_internal
fitted_values =  results.predict(X)
from matplotlib.pyplot import plot, scatter, show, xlabel, ylabel
scatter(fitted_values,sqrt_residues_std)
xlabel("Fitted values")
ylabel("Racine carrée des résidus standardisés")
# en reprenant la fonction smooth() plus haut
xs , ys = smooth(fitted_values,sqrt_residues_std,box_percent=0.25,res=30)
plot(xs,ys,"-r")
plt.savefig('/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/reg_homoscédasticité.png')
show()

"""####1) Les individus sont indépendantes

"""

from statsmodels.graphics.tsaplots import plot_acf
plot_acf(res)

"""#### 2) Les résidus suivent une distribution normale ( Shapiro )"""

## Test de shapiro
st.shapiro(res_pearson)

## Test de kolmogorov
ks_2samp(results.resid,list(np.random.normal(np.mean(res), np.std(res), len(data_agg))))

##Visualisation
plt.hist(res,100)
plt.ylabel('Count')
plt.xlabel('Normalized residuals')
plt.xticks(fontsize=8)
plt.savefig('/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/reg_normalplot.png')
plt.show()

## QQ plot
residues_std = results.get_influence().resid_studentized_internal
fig = sm.qqplot(residues_std, st.t, fit=True,line='45')
plt.title("QQ plot")
plt.savefig('/content/drive/My Drive/DataAnalyst/P7_Muller_Gauthier/rendu/reg_qqplot.png')
plt.show()